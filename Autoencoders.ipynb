{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import random as rnd\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from itertools import product, combinations\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import project, NPR, MF2PCA2ORIG, cross_val_score_custom, filter_paths\n",
    "import multiprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ray import tune\n",
    "import hyperopt as hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-10\n",
    "\n",
    "def mae_score(y, y_pred):\n",
    "    N = y.shape[0]\n",
    "    eps = np.ones((N))*EPS\n",
    "    denominator =  np.max(np.stack([np.linalg.norm(y, axis=1, ord=1), eps], 1), 1)\n",
    "    return (np.linalg.norm(y_pred - y, axis=1, ord=1) / denominator).mean()\n",
    "\n",
    "def mae_score_torch(y, y_pred):\n",
    "    N = y.shape[0]\n",
    "    eps = torch.ones(N, device=y.device)*EPS\n",
    "    denominator =  torch.max(torch.stack([torch.norm(y, dim=1, p=1), eps], 1), 1)[0]\n",
    "    return (torch.norm(y_pred - y, dim=1, p=1) / denominator).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED = False\n",
    "SAVE = True\n",
    "\n",
    "if MERGED:\n",
    "    root = './merged_datasets_proj/'\n",
    "    paths = [os.path.join(root,path) for path in os.listdir(root)]\n",
    "    intrinsic_dims = np.load('intrinsic_dims_merged.npy', allow_pickle=True).item()\n",
    "else:\n",
    "    root = './separate_datasets_proj/'\n",
    "    DATASETS = ['AGP', 'ptb']\n",
    "    N_DATASETS = len(DATASETS)\n",
    "    paths = filter_paths([os.path.join(root,path) for path in os.listdir(root)], keywords=DATASETS)\n",
    "    root = './separate_datasets_proj/'\n",
    "    intrinsic_dims = np.load('intrinsic_dims_separate.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, z_dim, config):\n",
    "        hidden_dim = config['hidden_dim']\n",
    "        dropout_rate1 = config['dropout_rate1']\n",
    "        dropout_rate2 = config['dropout_rate2']\n",
    "        dropout_rate3 = config['dropout_rate3']\n",
    "        dropout_rate4 = config['dropout_rate4']\n",
    "        \n",
    "        dropout_rate5 = config['dropout_rate5']\n",
    "        dropout_rate6 = config['dropout_rate6']\n",
    "        dropout_rate7 = config['dropout_rate7']\n",
    "        dropout_rate8 = config['dropout_rate8']\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                    nn.BatchNorm1d(hidden_dim),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate1),\n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.BatchNorm1d(hidden_dim),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate2), \n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim, hidden_dim//2),\n",
    "                                    nn.BatchNorm1d(hidden_dim//2), \n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate3),\n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "                                    nn.BatchNorm1d(hidden_dim//4),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate4), \n",
    "                                    nn.Linear(hidden_dim//4, z_dim))\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Linear(z_dim, hidden_dim//4),\n",
    "                                    nn.BatchNorm1d(hidden_dim//4),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate5),\n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim//4, hidden_dim//2),\n",
    "                                    nn.BatchNorm1d(hidden_dim//2),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate6), \n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim//2, hidden_dim),\n",
    "                                    nn.BatchNorm1d(hidden_dim), \n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate7),\n",
    "                                     \n",
    "                                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.BatchNorm1d(hidden_dim),\n",
    "                                    nn.LeakyReLU(),\n",
    "                                    nn.Dropout(p=dropout_rate8), \n",
    "                                    nn.Linear(hidden_dim, input_dim))\n",
    "    def forward(self, X):\n",
    "        z = self.encoder(X)\n",
    "        X = self.decoder(z)\n",
    "        return z,X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(data_train, \n",
    "             data_test, \n",
    "             dim_range,\n",
    "             config,\n",
    "             n_epochs=6000,\n",
    "             batch_size=256,\n",
    "             validate=True,\n",
    "             calc_npr=True, \n",
    "             npr_calc_step=100,\n",
    "             calc_pdist=False,\n",
    "             pdist_calc_step=500,\n",
    "             return_history=True):\n",
    "    \n",
    "    min_loss_val_dim = []\n",
    "    if return_history:\n",
    "        results = defaultdict(list)\n",
    "        metrics_dict = defaultdict(dict)\n",
    "        \n",
    "    data_proj_train_torch = torch.tensor(data_train, dtype=torch.float).cuda()\n",
    "    data_proj_test_torch = torch.tensor(data_test, dtype=torch.float).cuda()\n",
    "    dist_train = torch.cdist(data_proj_train_torch, data_proj_train_torch, p=1)\n",
    "    dist_val = torch.cdist(data_proj_test_torch, data_proj_train_torch, p=1)\n",
    "    \n",
    "    for dim in tqdm_notebook(dim_range):\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        loss_train = []\n",
    "        loss_val = []\n",
    "        npr_train = []\n",
    "        npr_val = []\n",
    "        pdist_loss_list_train = []\n",
    "        pdist_loss_list_val = []\n",
    "\n",
    "        ae = AE(data_train.shape[1], dim, config).cuda()\n",
    "        opt = torch.optim.Adam(ae.parameters(), \n",
    "                               lr=config['lr'], \n",
    "                               weight_decay=config['weight_decay'], \n",
    "                               betas=(config['beta1'], config['beta2'])) \n",
    "\n",
    "        dataloader = DataLoader(data_proj_train_torch, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "        for i in tqdm_notebook(range(n_epochs)):\n",
    "            #########\n",
    "            # TRAIN #\n",
    "            #########\n",
    "            ae.train()\n",
    "            ae_input = data_proj_train_torch + 1e-2*torch.randn_like(data_proj_train_torch).cuda() # ADD Noise\n",
    "            embedding, data_rec = ae(ae_input)\n",
    "            \n",
    "            if return_history:\n",
    "                if i%npr_calc_step==0 and calc_npr:\n",
    "                    npr_train.append(NPR(data_train, \n",
    "                                         embedding.detach().cpu().numpy()))\n",
    "\n",
    "                if i%pdist_calc_step==0 and calc_pdist:\n",
    "                    dist_emb = torch.cdist(embedding, embedding, p=1)\n",
    "                    pdist_loss_train = (dist_emb - dist_train).abs().sum()\n",
    "                    pdist_loss_list_train.append(pdist_loss_train.item())\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss = mae_score_torch(data_proj_train_torch, data_rec) \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            loss_train.append(loss.item())\n",
    "    \n",
    "            ##############\n",
    "            # VALIDATION #\n",
    "            ##############\n",
    "            ae.eval()\n",
    "            if validate:\n",
    "                with torch.no_grad():\n",
    "                    embedding_, data_rec_test = ae(data_proj_test_torch)\n",
    "                    loss_ = mae_score_torch(data_proj_test_torch, data_rec_test)\n",
    "                    loss_val.append(loss_.item())\n",
    "                    \n",
    "                if return_history:\n",
    "                    if i%npr_calc_step==0 and calc_npr:\n",
    "                        npr_val.append(NPR(data_test, \n",
    "                                           embedding_.detach().cpu().numpy()))\n",
    "\n",
    "                    if i%pdist_calc_step==0 and calc_pdist:\n",
    "                        dist_val = torch.cdist(data_proj_test_torch, data_proj_test_torch, p=1)\n",
    "                        dist_emb_ = torch.cdist(embedding_, embedding_, p=1)\n",
    "                        pdist_loss_val = (dist_emb_ - dist_val).abs().sum()\n",
    "                        pdist_loss_list_val.append(pdist_loss_val.item())\n",
    "        \n",
    "        \n",
    "        min_loss_val_dim.append(min(loss_val))\n",
    "        if return_history:\n",
    "            metrics_dict[dim]['loss_train'] = loss_train\n",
    "            metrics_dict[dim]['loss_val'] = loss_val\n",
    "            if calc_npr:\n",
    "                metrics_dict[dim]['npr_train'] = npr_train\n",
    "                metrics_dict[dim]['npr_val'] = npr_val\n",
    "            if calc_pdist:\n",
    "                metrics_dict[dim]['pdist_loss_list_train'] = pdist_loss_list_train\n",
    "                metrics_dict[dim]['pdist_loss_list_val'] = pdist_loss_list_val\n",
    "\n",
    "            results['model'].append(ae)\n",
    "            results['optimizer'].append(opt)\n",
    "    \n",
    "    tune.report(mean_loss=float(min(min_loss_val_dim)))\n",
    "    if return_history:\n",
    "        results['min_loss_val_dim'] = min_loss_val_dim\n",
    "        results['metrics'] = metrics_dict\n",
    "        results['dim_range'] = dim_range\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_dataset(path, intrinsic_dims, config, choose_one_dim=True, return_history=True):\n",
    "    \n",
    "    data = np.genfromtxt(os.path.join('/nfs/hpc2_storage/ibulygin/Skoltech-Microbiome-Project', path), delimiter=';')\n",
    "    scaler = RobustScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    data_train, data_test = train_test_split(data, random_state=42)\n",
    "    \n",
    "    if choose_one_dim:\n",
    "        recommended_dims = intrinsic_dims[path]\n",
    "        # choose max dim over recommended by MLE\n",
    "        dim_range = [max(recommended_dims)]\n",
    "    else:\n",
    "        dim_range = np.arange(2,data.shape[1])\n",
    "        \n",
    "    results = train_ae(data_train, \n",
    "                       data_test, \n",
    "                       dim_range,\n",
    "                       config,\n",
    "                       n_epochs=10000,# 6000\n",
    "                       batch_size=500, # 200\n",
    "                       validate=True,\n",
    "                       calc_npr=False, \n",
    "                       npr_calc_step=100,\n",
    "                       calc_pdist=False,\n",
    "                       pdist_calc_step=500,\n",
    "                       return_history=return_history)\n",
    "    if return_history:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {'beta1':0.9,\n",
    "                  'beta2':0.9,\n",
    "                  'weight_decay':1e-5,\n",
    "                    'lr':2e-4,\n",
    "                    'hidden_dim': 1024,\n",
    "                    'dropout_rate1': 0.05,\n",
    "                    'dropout_rate2': 0.1,\n",
    "                    'dropout_rate3': 0.15,\n",
    "                    'dropout_rate4': 0.0,\n",
    "                    'dropout_rate5': 0.15,\n",
    "                    'dropout_rate6': 0.1,\n",
    "                    'dropout_rate7': 0.05,\n",
    "                    'dropout_rate8': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = train_on_dataset('./merged_datasets_proj/proj_{tax}.csv', \n",
    "#                            intrinsic_dims, \n",
    "#                            config=default_config, \n",
    "#                            choose_one_dim=True, \n",
    "#                            return_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in results['metrics'][9].items():\n",
    "#     d =results['min_loss_val_dim'][0]\n",
    "#     plt.plot(v, label=k)\n",
    "#     plt.yscale('log')\n",
    "#     plt.ylabel('MAPE Loss')\n",
    "#     plt.title(f'Min val loss: {d}')\n",
    "# plt.vlines(np.argmin(v), min(v), max(v), color='r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainable = lambda x: train_on_dataset('./merged_datasets_proj/proj_g.csv', intrinsic_dims, x, return_history=False)\n",
    "\n",
    "# # hyperopt = HyperOptSearch(metric=\"score\", mode=\"min\")\n",
    "\n",
    "# analysis = tune.run(\n",
    "#     trainable,\n",
    "#     config={'beta1':tune.uniform(0.5,0.9),\n",
    "#           'beta2':tune.uniform(0.5,0.99),\n",
    "#           'weight_decay':tune.grid_search([1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]),\n",
    "#             'lr':tune.grid_search([1e-1,1e-3]),\n",
    "#             'hidden_dim': tune.grid_search([200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700]),\n",
    "#             'dropout_rate1': tune.uniform(0.0,0.5),\n",
    "#             'dropout_rate2': tune.uniform(0.0,0.5),\n",
    "#             'dropout_rate3': tune.uniform(0.0,0.5),\n",
    "#             'dropout_rate4': tune.uniform(0.0,0.5),\n",
    "#             'dropout_rate5': tune.uniform(0.0,0.5),\n",
    "#             'dropout_rate6': tune.uniform(0.0,0.5),\n",
    "#             'dropout_rate7': tune.uniform(0.0,0.5),\n",
    "#             'dropout_rate8': tune.uniform(0.0,0.5)},\n",
    "#     resources_per_trial={'gpu': 1},\n",
    "#     num_samples=5\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls /home/ibulygin/ray_results/lambda_2021-02-11_15-34-56/lambda_8c66e_00000_0_beta1\\=0.61925,beta2\\=0.59225,dropout_rate1\\=0.040394,dropout_rate2\\=0.34461,dropout_rate3\\=0.20738,dropout_rate4\\=_2021-02-11_15-34-56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74bd615780848f5b92efda48f1e297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e1647527744446af2c77f12e401c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8697e0e30fa4ceaab66ab61f8db30bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session not detected. You should not be calling this function outside `tune.run` or while using the class API. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe130205572e452d92177ce71b8afbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bdcd5d044e49e698617d3a3a417a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session not detected. You should not be calling this function outside `tune.run` or while using the class API. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee08c4194c048e380621e1de4275278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a72f0c597ce457cbda12fcffe344fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session not detected. You should not be calling this function outside `tune.run` or while using the class API. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898adc17ec5a4a01b65ee48a901caa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0474715913204bcf89dda691ebaeacf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session not detected. You should not be calling this function outside `tune.run` or while using the class API. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b808bc23d24580b415d8bf3e41865e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c0025741ca490494940a37603ec063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session not detected. You should not be calling this function outside `tune.run` or while using the class API. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547fcdd3fdd6446d96858c2300c45179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e8a72a222744569eb98586adc96fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session not detected. You should not be calling this function outside `tune.run` or while using the class API. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for path in tqdm_notebook(paths):\n",
    "    results.append(train_on_dataset(path, \n",
    "                                    intrinsic_dims, \n",
    "                                    config=default_config, \n",
    "                                    choose_one_dim=True, \n",
    "                                    return_history=True))\n",
    "\n",
    "label2path = {}\n",
    "datasets_result = {}\n",
    "for path, result in zip(paths, results):\n",
    "    label = path.split('/')[-1].split('.')[0]\n",
    "    label2path[label] = path\n",
    "    datasets_result[label] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For AGP_proj_o, dim=[6], min_val_loss [0.15249169] achieved\n",
      "Saving...\n",
      "For AGP_proj_f, dim=[9], min_val_loss [0.21868196] achieved\n",
      "Saving...\n",
      "For AGP_proj_g, dim=[9], min_val_loss [0.25573888] achieved\n",
      "Saving...\n",
      "For ptb_proj_f, dim=[7], min_val_loss [0.24307558] achieved\n",
      "Saving...\n",
      "For ptb_proj_g, dim=[7], min_val_loss [0.24619679] achieved\n",
      "Saving...\n",
      "For ptb_proj_o, dim=[5], min_val_loss [0.15823671] achieved\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "for k,v in datasets_result.items():\n",
    "    \n",
    "    min_loss_val_dim = np.array(v['min_loss_val_dim'])\n",
    "    metrics_dict = v['metrics']\n",
    "    ae_list = v['model']\n",
    "    dim_range = v['dim_range']\n",
    "    \n",
    "    if len(dim_range) > 1:\n",
    "        \n",
    "        recommended_intrinsic_dims = intrinsic_dims[label2path[k]]\n",
    "        ind = np.intersect1d(dim_range, recommended_intrinsic_dims, assume_unique=True, return_indices=True)[1].tolist()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(dim_range, min_loss_val_dim)\n",
    "        plt.vlines(int_dims[[0,-1]], min_loss_val_dim.min(), min_loss_val_dim.max(), 'r', alpha=0.5)\n",
    "        plt.xlabel('latent dim')\n",
    "        plt.ylabel('MAPE')\n",
    "        plt.title(k)\n",
    "        \n",
    "        ae=ae_list\n",
    "    else:\n",
    "        print(f'For {k}, dim={dim_range}, min_val_loss {min_loss_val_dim} achieved')\n",
    "        ae=ae_list[0]\n",
    "        \n",
    "    if SAVE:\n",
    "        print('Saving...')\n",
    "        # make embedding\n",
    "        data = np.genfromtxt(label2path[k], delimiter=';')\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        embedding_all, _ = ae(torch.from_numpy(data_scaled).cuda().float())\n",
    "        embedding_all = embedding_all.detach().cpu().numpy()\n",
    "\n",
    "        if MERGED:\n",
    "            np.save(f'./merged_datasets_transformed/{k}_ae', embedding_all)\n",
    "        else:\n",
    "            np.save(f'./separate_datasets_transformed/{k}_ae', embedding_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive separate from merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_nums = {'AGP':9511, 'ptb':3457, 't2d':1044, 'ibd':86}\n",
    "# data = None # put dataset here\n",
    "# orig_indexes = np.arange(data.shape[0])\n",
    "# remained_items_mask = np.ones_like(orig_indexes)\n",
    "\n",
    "# v_prev = 0\n",
    "# item_iter = 0\n",
    "# for dataset_name,v in datasets_nums.items():\n",
    "#     n_items = sum(remained_items_mask[v_prev:v_prev+v])\n",
    "#     np.save(f'./separate_datasets_transformed/{dataset_name}_{k}_ae', embedding_all[item_iter:item_iter+n_items])\n",
    "#     v_prev += v\n",
    "#     item_iter += n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged \n",
    "\n",
    "Saving...For proj_f, dim=[8], min_val_loss [0.20790944] achieved  \n",
    "For proj_g, dim=[9], min_val_loss [0.2425641] achieved  \n",
    "For proj_o, dim=[6], min_val_loss [0.11661357] achieved  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
